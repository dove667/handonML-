import torch
import argparse
from transformers import BertTokenizer, BertForQuestionAnswering, Trainer, TrainingArguments, default_data_collator
from datasets import load_dataset
import numpy as np
import os
from tqdm.auto import tqdm
from collections import defaultdict

def main():
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="åœ¨ SQuAD é—®ç­”ä»»åŠ¡ä¸Šå¾®è°ƒ BERT æ¨¡å‹ã€‚")
    parser.add_argument("--mode", type=str, choices=["train", "test"], required=True,
                        help="è¿è¡Œæ¨¡å¼ï¼š'train' (è®­ç»ƒ) æˆ– 'test' (æµ‹è¯•)")
    parser.add_argument("--epochs", type=int, default=3,
                        help="æ€»è®­ç»ƒè½®æ•° (ä»…åœ¨ train æ¨¡å¼ä¸‹æœ‰æ•ˆ)ã€‚")
    parser.add_argument("--epoch", type=int, default=None,
                        help="æµ‹è¯•æ¨¡å¼ä¸‹è¦åŠ è½½çš„ checkpoint å¯¹åº”çš„ epoch ç¼–å·ã€‚")
    parser.add_argument("--train_batch_size", type=int, default=8,
                        help="è®­ç»ƒæ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°ã€‚")
    parser.add_argument("--eval_batch_size", type=int, default=16,
                        help="è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°ã€‚")
    parser.add_argument("--max_seq_length", type=int, default=384,
                        help="åˆ†è¯åè¾“å…¥åºåˆ—çš„æœ€å¤§æ€»é•¿åº¦ã€‚")
    parser.add_argument("--doc_stride", type=int, default=128,
                        help="å½“æ–‡æ¡£è¿‡é•¿æ—¶ï¼Œåˆ†å‰²æˆè¾ƒçŸ­ç‰‡æ®µæ—¶çš„æ­¥é•¿ã€‚")

    args = parser.parse_args()
    
    output_dir = "model/Bert/"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. æ•°æ®åŠ è½½ä¸æ¢ç´¢
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    # ä½¿ç”¨ SQuAD v2 æ•°æ®é›†è¿›è¡Œé—®ç­”ä»»åŠ¡
    # SQuAD v2 åŒ…å«æœ‰ç­”æ¡ˆå’Œæ— ç­”æ¡ˆçš„é—®é¢˜ï¼Œæ›´å…·æŒ‘æˆ˜æ€§
    dataset = load_dataset("squad_v2")
    train_dataset = dataset["train"]
    validation_dataset = dataset["validation"]

    print("è®­ç»ƒé›†å¤§å°:", len(train_dataset))
    print("éªŒè¯é›†å¤§å°:", len(validation_dataset))

    print("\nè®­ç»ƒé›†ç¤ºä¾‹:")
    print(train_dataset[0])

    # 2. æ•°æ®é¢„å¤„ç†ä¸åˆ†è¯
    # åŠ è½½ BERT çš„åˆ†è¯å™¨
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # SQuAD è®­ç»ƒæ•°æ®é¢„å¤„ç†å‡½æ•°
    def prepare_train_features(examples):
        # å¯¹ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨æˆªæ–­å’Œå¡«å……ï¼Œä½†ä¿ç•™æº¢å‡ºçš„ token ä»¥å¤„ç†é•¿æ–‡æ¡£ã€‚
        # è¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹ï¼ˆé—®é¢˜+ä¸Šä¸‹æ–‡ï¼‰å¯èƒ½ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰éƒ¨åˆ†é‡å ã€‚
        # è¿™æ ·å¯ä»¥ç¡®ä¿é•¿æ–‡æ¡£ä¸­çš„ç­”æ¡ˆä¸ä¼šå› ä¸ºæˆªæ–­è€Œä¸¢å¤±ã€‚
        pad_on_right = tokenizer.padding_side == "right" # åˆ¤æ–­å¡«å……æ–¹å‘ï¼ŒBERT é€šå¸¸æ˜¯å³å¡«å……

        # å¯¹é—®é¢˜å’Œä¸Šä¸‹æ–‡è¿›è¡Œåˆ†è¯
        tokenized_examples = tokenizer(
            examples["question" if pad_on_right else "context"], # æ ¹æ®å¡«å……æ–¹å‘ç¡®å®šå…ˆå¤„ç†é—®é¢˜è¿˜æ˜¯ä¸Šä¸‹æ–‡
            examples["context" if pad_on_right else "question"],
            truncation="only_second" if pad_on_right else "only_first", # ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ï¼ˆé€šå¸¸æ˜¯ä¸Šä¸‹æ–‡ï¼‰
            max_length=args.max_seq_length, # æœ€å¤§åºåˆ—é•¿åº¦
            stride=args.doc_stride, # æ­¥é•¿ï¼Œç”¨äºå¤„ç†é•¿æ–‡æ¡£æ—¶çš„é‡å 
            return_overflowing_tokens=True, # è¿”å›æº¢å‡ºçš„ token
            return_offsets_mapping=True, # è¿”å› token åˆ°åŸå§‹æ–‡æœ¬çš„åç§»æ˜ å°„
            padding="max_length", # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        )

        # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ˜ å°„æ¥çŸ¥é“æ¯ä¸ªç‰¹å¾å¯¹åº”å“ªä¸ªåŸå§‹ç¤ºä¾‹ã€‚
        # overflow_to_sample_mapping æä¾›äº†è¿™ä¸ªæ˜ å°„ã€‚
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
        # åç§»æ˜ å°„ (offset_mapping) æä¾›äº†è¿æ¥åçš„ token åºåˆ—ä¸­æ¯ä¸ª token å¯¹åº”åœ¨åŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦ä½ç½®ã€‚
        # è¿™å¯¹äºè®¡ç®—ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ token ä½ç½®éå¸¸æœ‰ç”¨ã€‚
        offset_mapping = tokenized_examples.pop("offset_mapping")

        # ä¸ºè¿™äº›ç‰¹å¾æ ‡æ³¨ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼
        tokenized_examples["start_positions"] = []
        tokenized_examples["end_positions"] = []

        # éå†æ¯ä¸ªç”Ÿæˆçš„ç‰¹å¾
        for i, offsets in enumerate(offset_mapping):
            # å¯¹äºæ— æ³•å›ç­”çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†ç­”æ¡ˆä½ç½®æ ‡è®°ä¸º [CLS] token çš„ç´¢å¼•ã€‚
            input_ids = tokenized_examples["input_ids"][i]
            cls_index = input_ids.index(tokenizer.cls_token_id) # [CLS] token çš„ç´¢å¼•

            # è·å–å¯¹åº”äºä¸Šä¸‹æ–‡çš„åºåˆ—ï¼ˆæ ¹æ® token type IDsï¼‰ã€‚
            # BERT è¾“å…¥é€šå¸¸æ˜¯ [CLS] é—®é¢˜ [SEP] ä¸Šä¸‹æ–‡ [SEP]ï¼Œtoken type IDs ç”¨äºåŒºåˆ†é—®é¢˜å’Œä¸Šä¸‹æ–‡ã€‚
            sequence_ids = tokenized_examples.sequence_ids(i)

            # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªè·¨åº¦ï¼ˆç‰¹å¾ï¼‰ï¼Œè¿™æ˜¯åŒ…å«å½“å‰æ–‡æœ¬è·¨åº¦çš„åŸå§‹ç¤ºä¾‹çš„ç´¢å¼•ã€‚
            sample_index = sample_mapping[i]
            answers = examples["answers"][sample_index] # è·å–åŸå§‹ç¤ºä¾‹çš„ç­”æ¡ˆä¿¡æ¯

            # å¦‚æœæ²¡æœ‰æä¾›ç­”æ¡ˆï¼ˆSQuAD v2 ä¸­çš„æ— ç­”æ¡ˆé—®é¢˜ï¼‰ï¼Œå°†ç­”æ¡ˆä½ç½®è®¾ç½®ä¸º [CLS] token çš„ç´¢å¼•ã€‚
            if len(answers["answer_start"]) == 0:
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # ç­”æ¡ˆåœ¨åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„èµ·å§‹/ç»“æŸå­—ç¬¦ç´¢å¼•
                start_char = answers["answer_start"][0]
                end_char = start_char + len(answers["text"][0])

                # å½“å‰è·¨åº¦ï¼ˆç‰¹å¾ï¼‰åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹å’Œç»“æŸ token ç´¢å¼•ã€‚
                token_start_index = 0
                # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„èµ·å§‹ token ç´¢å¼•
                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                    token_start_index += 1
                token_end_index = len(input_ids) - 1
                # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„ç»“æŸ token ç´¢å¼•
                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                    token_end_index -= 1

                # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºå½“å‰è·¨åº¦ï¼ˆç‰¹å¾ï¼‰çš„èŒƒå›´ã€‚
                # å¦‚æœç­”æ¡ˆçš„èµ·å§‹å­—ç¬¦ä½ç½®å°äºå½“å‰è·¨åº¦ç¬¬ä¸€ä¸ª token çš„èµ·å§‹å­—ç¬¦ä½ç½®ï¼Œ
                # æˆ–è€…ç­”æ¡ˆçš„ç»“æŸå­—ç¬¦ä½ç½®å¤§äºå½“å‰è·¨åº¦æœ€åä¸€ä¸ª token çš„ç»“æŸå­—ç¬¦ä½ç½®ï¼Œ
                # åˆ™è®¤ä¸ºç­”æ¡ˆè¶…å‡ºèŒƒå›´ï¼Œå°†å…¶æ ‡è®°ä¸º [CLS] ç´¢å¼•ã€‚
                if start_char < offsets[token_start_index][0] or end_char > offsets[token_end_index][1]:
                    tokenized_examples["start_positions"].append(cls_index)
                    tokenized_examples["end_positions"].append(cls_index)
                else:
                    # å¦åˆ™ï¼Œå°† token_start_index å’Œ token_end_index ç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ä¸ªç«¯ç‚¹ã€‚
                    # æ³¨æ„ï¼šæˆ‘ä»¬å°† token_start_index å‘åç§»åŠ¨ 1ï¼Œå› ä¸ºä¸Šä¸‹æ–‡çš„ç¬¬ä¸€ä¸ª token é€šå¸¸åœ¨ç´¢å¼• 1
                    # ï¼ˆç´¢å¼• 0 æ˜¯ [CLS] tokenï¼‰ã€‚
                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                        token_start_index += 1
                    tokenized_examples["start_positions"].append(token_start_index - 1)
                    while offsets[token_end_index][1] >= end_char:
                        token_end_index -= 1
                    tokenized_examples["end_positions"].append(token_end_index + 1)

        return tokenized_examples

    # SQuAD éªŒè¯æ•°æ®é¢„å¤„ç†å‡½æ•°
    def prepare_validation_features(examples):
        # é¢„å¤„ç†é€»è¾‘ä¸è®­ç»ƒé›†ç±»ä¼¼ï¼Œä½†ä¸éœ€è¦æ ‡æ³¨ç­”æ¡ˆä½ç½®ï¼Œè€Œæ˜¯ä¿ç•™åŸå§‹ç¤ºä¾‹ ID å’Œåç§»æ˜ å°„ï¼Œä»¥ä¾¿åç»­è¿›è¡Œåå¤„ç†å’Œè¯„ä¼°ã€‚
        pad_on_right = tokenizer.padding_side == "right"
        tokenized_examples = tokenizer(
            examples["question" if pad_on_right else "context"],
            examples["context" if pad_on_right else "question"],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=args.max_seq_length,
            stride=args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )

        # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ˜ å°„æ¥çŸ¥é“æ¯ä¸ªç‰¹å¾å¯¹åº”å“ªä¸ªåŸå§‹ç¤ºä¾‹ã€‚
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        # ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬éœ€è¦å°†é¢„æµ‹ç»“æœè½¬æ¢å›ç­”æ¡ˆæ–‡æœ¬ï¼Œè¿™éœ€è¦è®¿é—®åŸå§‹ç¤ºä¾‹ã€‚
        # æˆ‘ä»¬å°†åŸå§‹ç¤ºä¾‹çš„ ID å­˜å‚¨åœ¨åˆ†è¯åçš„æ•°æ®é›†ä¸­ã€‚
        tokenized_examples["example_id"] = [examples["id"][sample_mapping[i]] for i in range(len(tokenized_examples["input_ids"]))]

        # æˆ‘ä»¬è¿˜éœ€è¦å­˜å‚¨åç§»æ˜ å°„ç”¨äºè¯„ä¼°ã€‚
        tokenized_examples["offset_mapping"] = tokenized_examples.pop("offset_mapping")

        return tokenized_examples


    print("\næ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...")
    # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†åº”ç”¨é¢„å¤„ç†å‡½æ•°
    tokenized_train_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)
    tokenized_validation_dataset = validation_dataset.map(prepare_validation_features, batched=True, remove_columns=validation_dataset.column_names)

    # 3. DataLoaderæ„å»º (Trainer å†…éƒ¨å¤„ç†)
    # æ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºå°†æ‰¹æ¬¡å†…çš„è¾“å…¥å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦
    data_collator = default_data_collator

    # 4. æ¨¡å‹å®šä¹‰
    print("\næ­£åœ¨åŠ è½½ BERT æ¨¡å‹...")
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ä¸”æŒ‡å®šäº† epochï¼Œåˆ™ä» checkpoint åŠ è½½æ¨¡å‹
    if args.mode == "test" and args.epoch is not None:
        # æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ epoch æ„å»º checkpoint è·¯å¾„
        checkpoint_path = os.path.join(output_dir, f"checkpoint-{args.epoch}")
        if not os.path.exists(checkpoint_path):
             print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° checkpoint è·¯å¾„ {checkpoint_path}ã€‚è¯·æ£€æŸ¥è·¯å¾„å’Œ epoch ç¼–å·æ˜¯å¦æ­£ç¡®ã€‚")
             return
        model = BertForQuestionAnswering.from_pretrained(checkpoint_path)
        print(f"å·²ä» checkpoint åŠ è½½æ¨¡å‹ï¼š{checkpoint_path}")
    else:
        # å¦åˆ™åŠ è½½é¢„è®­ç»ƒçš„ bert-base-uncased æ¨¡å‹
        model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")
        print("å·²åŠ è½½åŸºç¡€ BERT æ¨¡å‹ã€‚")

    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡ (é—®ç­”ä»»åŠ¡çš„è¯„ä¼°æ¯”è¾ƒå¤æ‚ï¼Œé€šå¸¸åœ¨é¢„æµ‹åè¿›è¡Œ)
    # æˆ‘ä»¬å°†ä½¿ç”¨éªŒè¯é›† loss ä½œä¸ºè®­ç»ƒæœŸé—´ç›‘æ§çš„æŒ‡æ ‡ã€‚

    # 5. è®­ç»ƒä¸è¯„ä¼°
    print("\næ­£åœ¨è®¾ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=output_dir,           # è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜ checkpoint å’Œè®­ç»ƒç»“æœ
        num_train_epochs=args.epochs,    # æ€»è®­ç»ƒè½®æ•°
        per_device_train_batch_size=args.train_batch_size,  # è®­ç»ƒæ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size=args.eval_batch_size,   # è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
        warmup_steps=500,                # å­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥æ•°
        weight_decay=0.01,               # æƒé‡è¡°å‡å¼ºåº¦
        logging_dir="runs/Bert",         # æ—¥å¿—å­˜å‚¨ç›®å½•
        logging_steps=10,                # æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        evaluation_strategy="epoch" if args.mode == "train" else "no", # è®­ç»ƒæ¨¡å¼ä¸‹æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
        save_strategy="epoch" if args.mode == "train" else "no",       # è®­ç»ƒæ¨¡å¼ä¸‹æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpoint
        load_best_model_at_end=True if args.mode == "train" else False,# è®­ç»ƒç»“æŸæ—¶åŠ è½½éªŒè¯é›†è¡¨ç°æœ€ä½³çš„æ¨¡å‹
        metric_for_best_model="eval_loss", # ä½¿ç”¨éªŒè¯é›† loss ä½œä¸ºåˆ¤æ–­æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
        greater_is_better=False,         # loss è¶Šä½è¶Šå¥½
        report_to="none"                 # ç¦ç”¨å‘å¤–éƒ¨æœåŠ¡ï¼ˆå¦‚ W&Bï¼‰æŠ¥å‘Š
    )

    print("æ­£åœ¨åˆå§‹åŒ– Trainer...")
    # å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šå¸¸ä¸åœ¨ Trainer çš„è®­ç»ƒ/è¯„ä¼°å¾ªç¯ä¸­ç›´æ¥è®¡ç®— EM/F1 ç­‰æŒ‡æ ‡ã€‚
    # è¯„ä¼°æ˜¯é€šè¿‡é¢„æµ‹èµ·å§‹/ç»“æŸä½ç½®ï¼Œç„¶åæ˜ å°„å›æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨å®˜æ–¹çš„ SQuAD è¯„ä¼°è„šæœ¬æ¥å®Œæˆçš„ã€‚
    # åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å°†ç›‘æ§éªŒè¯é›† lossã€‚
    trainer = Trainer(
        model=model,                         # è¦è®­ç»ƒçš„ ğŸ¤— Transformers æ¨¡å‹å®ä¾‹
        args=training_args,                  # è®­ç»ƒå‚æ•°
        train_dataset=tokenized_train_dataset if args.mode == "train" else None, # è®­ç»ƒæ•°æ®é›†
        eval_dataset=tokenized_validation_dataset if args.mode == "train" else None, # è¯„ä¼°æ•°æ®é›†
        tokenizer=tokenizer,                 # åˆ†è¯å™¨ï¼Œç”¨äºæ•°æ®æ”¶é›†å™¨
        data_collator=data_collator,         # æ•°æ®æ”¶é›†å™¨
    )

    if args.mode == "train":
        print("æ­£åœ¨å¼€å§‹è®­ç»ƒ...")
        # Trainer ä¼šæ ¹æ® save_strategy å’Œ metric_for_best_model è‡ªåŠ¨ä¿å­˜ checkpointã€‚
        # å®ƒä¼šä¿å­˜åˆ° output_dir/checkpoint-step_number å’Œ output_dir/best_model ç›®å½•ä¸‹ã€‚
        # Trainer å†…éƒ¨å·²ç»åŒ…å«äº†æ ¹æ® eval_loss ä¿å­˜æœ€ä½³æ¨¡å‹çš„åŠŸèƒ½ã€‚
        trainer.train()
        print("\nè®­ç»ƒå®Œæˆã€‚")
        print(f"Checkpoint ä¿å­˜åœ¨ {output_dir} ç›®å½•ä¸‹ã€‚")

    elif args.mode == "test":
        # æµ‹è¯•æ¨¡å¼ä¸‹çš„è‡ªå®šä¹‰äº¤äº’å¼é—®ç­”å¾ªç¯
        if args.epoch is None:
            print("é”™è¯¯ï¼šæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»é€šè¿‡ --epoch å‚æ•°æŒ‡å®šè¦åŠ è½½çš„ checkpoint ç¼–å·ã€‚")
            return

        print(f"æ­£åœ¨ä½¿ç”¨ epoch {args.epoch} çš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼æµ‹è¯•...")

        # éœ€è¦åŸå§‹éªŒè¯æ•°æ®é›†ç”¨äºè·å–ä¸Šä¸‹æ–‡å’Œè¿›è¡Œæ˜ å°„
        validation_features = tokenized_validation_dataset
        validation_examples = validation_dataset

        # è·å–é¢„æµ‹ç»“æœ
        print("æ­£åœ¨éªŒè¯é›†ä¸Šè·å–é¢„æµ‹ç»“æœ...")
        # Trainer.predict è¿”å›ä¸€ä¸ª PredictionOutput å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«é¢„æµ‹çš„ logits
        predictions = trainer.predict(validation_features)
        start_logits, end_logits = predictions.predictions

        # é—®ç­”é¢„æµ‹ç»“æœåå¤„ç†
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„åå¤„ç†ç‰ˆæœ¬ï¼Œå®˜æ–¹çš„ SQuAD è¯„ä¼°è„šæœ¬æ›´å¥å£®
        def postprocess_qa_predictions(examples, features, start_logits, end_logits, n_best_size=20, max_answer_length=30):
            # æ„å»ºä¸€ä¸ªä» example ID åˆ°å…¶å¯¹åº” features ç´¢å¼•åˆ—è¡¨çš„æ˜ å°„
            example_to_features = defaultdict(list)
            for idx, feature in enumerate(features):
                example_to_features[feature["example_id"]].append(idx)

            # å­˜å‚¨æœ€ç»ˆé¢„æµ‹ç»“æœçš„å­—å…¸
            all_predictions = {}

            # éå†æ‰€æœ‰åŸå§‹ç¤ºä¾‹
            for example_index, example in enumerate(tqdm(examples)):
                # è·å–ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾ç´¢å¼•åˆ—è¡¨
                feature_indices = example_to_features[example["id"]]

                min_null_score = None # ç”¨äº SQuAD v2ï¼Œè®°å½•æ— ç­”æ¡ˆé¢„æµ‹çš„å¾—åˆ†
                valid_answers = [] # å­˜å‚¨å½“å‰ç¤ºä¾‹æ‰€æœ‰æœ‰æ•ˆç­”æ¡ˆè·¨åº¦çš„ä¿¡æ¯

                context = example["context"] # åŸå§‹ä¸Šä¸‹æ–‡æ–‡æœ¬

                # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ï¼Œæ‰¾åˆ°æœ€ä½³ç­”æ¡ˆè·¨åº¦
                for feature_index in feature_indices:
                    # è·å–å½“å‰ç‰¹å¾çš„èµ·å§‹å’Œç»“æŸ logits
                    start_logit = start_logits[feature_index]
                    end_logit = end_logits[feature_index]
                    # å½“å‰ç‰¹å¾çš„åç§»æ˜ å°„
                    offset_mapping = features[feature_index]["offset_mapping"]

                    # æ›´æ–°æœ€å°æ— ç­”æ¡ˆå¾—åˆ† (ä»… SQuAD v2 éœ€è¦)
                    # æ— ç­”æ¡ˆå¾—åˆ†æ˜¯ [CLS] token çš„èµ·å§‹å’Œç»“æŸ logits ä¹‹å’Œ
                    cls_index = features[feature_index]["input_ids"].index(tokenizer.cls_token_id)
                    feature_null_score = start_logit[cls_index] + end_logit[cls_index]
                    if min_null_score is None or min_null_score < feature_null_score:
                        min_null_score = feature_null_score

                    # æ‰¾åˆ° logits æœ€é«˜çš„ n_best_size ä¸ªèµ·å§‹å’Œç»“æŸä½ç½®
                    start_indexes = np.argsort(start_logit)[-1 : -n_best_size - 1 : -1].tolist()
                    end_indexes = np.argsort(end_logit)[-1 : -n_best_size - 1 : -1].tolist()

                    # éå†æ‰€æœ‰å¯èƒ½çš„èµ·å§‹å’Œç»“æŸä½ç½®ç»„åˆ
                    for start_index in start_indexes:
                        for end_index in end_indexes:
                            # è¿‡æ»¤æ‰æ— æ•ˆçš„ç­”æ¡ˆè·¨åº¦ï¼š
                            # 1. ç´¢å¼•è¶…å‡ºåç§»æ˜ å°„èŒƒå›´
                            # 2. åç§»æ˜ å°„ä¸º None (é€šå¸¸æ˜¯ç‰¹æ®Š token)
                            if (
                                start_index >= len(offset_mapping)
                                or end_index >= len(offset_mapping)
                                or offset_mapping[start_index] is None
                                or offset_mapping[end_index] is None
                            ):
                                continue
                            # 3. ç»“æŸç´¢å¼•å°äºèµ·å§‹ç´¢å¼• (è·¨åº¦æ— æ•ˆ)
                            # 4. ç­”æ¡ˆé•¿åº¦è¶…è¿‡æœ€å¤§å…è®¸é•¿åº¦
                            if end_index < start_index or end_index - start_index + 1 > max_answer_length:
                                continue

                            # è·å–ç­”æ¡ˆåœ¨åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦èµ·å§‹å’Œç»“æŸä½ç½®
                            start_char = offset_mapping[start_index][0]
                            end_char = offset_mapping[end_index][1]

                            # å°†æœ‰æ•ˆç­”æ¡ˆè·¨åº¦åŠå…¶å¾—åˆ†ã€æ–‡æœ¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                            valid_answers.append(
                                {"offsets": (start_char, end_char), "score": start_logit[start_index] + end_logit[end_index], "text": context[start_char: end_char]}
                            )

                # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆç­”æ¡ˆï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä½œä¸ºå½“å‰ç¤ºä¾‹çš„æœ€ä½³ç­”æ¡ˆ
                if len(valid_answers) > 0:
                    best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆç­”æ¡ˆï¼Œå°†æœ€ä½³ç­”æ¡ˆè®¾ç½®ä¸ºç©ºæ–‡æœ¬å’Œå¾—åˆ†ä¸º 0
                    best_answer = {"text": "", "score": 0.0}

                # å¦‚æœæ— ç­”æ¡ˆé¢„æµ‹çš„å¾—åˆ†é«˜äºæœ€ä½³ç­”æ¡ˆçš„å¾—åˆ†ï¼Œåˆ™é¢„æµ‹ç»“æœä¸ºç©ºå­—ç¬¦ä¸²ï¼ˆè¡¨ç¤ºæ— ç­”æ¡ˆï¼‰
                if min_null_score is not None and min_null_score > best_answer["score"]:
                     all_predictions[example["id"]] = ""
                else:
                    # å¦åˆ™ï¼Œå°†æœ€ä½³ç­”æ¡ˆçš„æ–‡æœ¬ä½œä¸ºé¢„æµ‹ç»“æœ
                    all_predictions[example["id"]] = best_answer["text"]

            return all_predictions

        # è·å–æ–‡æœ¬æ ¼å¼çš„é¢„æµ‹ç»“æœ
        print("æ­£åœ¨è¿›è¡Œé¢„æµ‹ç»“æœåå¤„ç†...")
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨ postprocess_qa_predictions å‡½æ•°ï¼Œä½†å…¶ç»“æœ all_predictions å¹¶æ²¡æœ‰è¢«ä½¿ç”¨ã€‚
        # è¿™éƒ¨åˆ†ä»£ç ä¸»è¦æ˜¯ä¸ºäº†æ¼”ç¤ºå¦‚ä½•è¿›è¡Œåå¤„ç†ï¼Œå®é™…äº¤äº’æµ‹è¯•ä½¿ç”¨çš„æ˜¯ä¸‹é¢çš„ä»£ç ã€‚
        # å¦‚æœéœ€è¦å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ (EM/F1)ï¼Œéœ€è¦ä½¿ç”¨å®˜æ–¹çš„ SQuAD è¯„ä¼°è„šæœ¬å¹¶ä¼ å…¥ all_predictionsã€‚
        # predictions_text = postprocess_qa_predictions(validation_examples, validation_features, start_logits, end_logits)


        # äº¤äº’å¼æµ‹è¯•å¾ªç¯
        print("\nè¿›å…¥äº¤äº’å¼æµ‹è¯•æ¨¡å¼ã€‚è¾“å…¥ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ¥æµ‹è¯•æ¨¡å‹ã€‚è¾“å…¥ 'quit' é€€å‡ºã€‚")
        while True:
            context_input = input("\nè¯·è¾“å…¥ä¸Šä¸‹æ–‡: ")
            if context_input.lower() == 'quit':
                break
            question_input = input("è¯·è¾“å…¥é—®é¢˜: ")
            if question_input.lower() == 'quit':
                break

            # å¯¹è‡ªå®šä¹‰è¾“å…¥è¿›è¡Œåˆ†è¯
            inputs = tokenizer(
                question_input,
                context_input,
                add_special_tokens=True, # æ·»åŠ  [CLS] å’Œ [SEP] ç­‰ç‰¹æ®Š token
                return_tensors="pt", # è¿”å› PyTorch å¼ é‡
                max_length=args.max_seq_length, # æœ€å¤§åºåˆ—é•¿åº¦
                truncation="only_second", # ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ï¼ˆä¸Šä¸‹æ–‡ï¼‰
                padding="max_length" # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            )

            # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ (CPU æˆ– GPU)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model.to(device)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # è·å–æ¨¡å‹çš„é¢„æµ‹ç»“æœ (logits)
            with torch.no_grad(): # åœ¨æ¨ç†æ¨¡å¼ä¸‹ï¼Œä¸è®¡ç®—æ¢¯åº¦
                outputs = model(**inputs)

            # è·å–èµ·å§‹å’Œç»“æŸä½ç½®çš„ logits
            start_logits = outputs.start_logits
            end_logits = outputs.end_logits

            # è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆè·¨åº¦çš„èµ·å§‹å’Œç»“æŸ token ç´¢å¼•
            answer_start_index = torch.argmax(start_logits)
            answer_end_index = torch.argmax(end_logits)

            # å°†è¾“å…¥ ID è½¬æ¢å› token åˆ—è¡¨
            input_ids = inputs["input_ids"].squeeze().tolist() # ç§»é™¤æ‰¹æ¬¡ç»´åº¦å¹¶è½¬æ¢ä¸ºåˆ—è¡¨
            tokens = tokenizer.convert_ids_to_tokens(input_ids) # å°† ID è½¬æ¢ä¸º token å­—ç¬¦ä¸²

            # è·å–ç­”æ¡ˆè·¨åº¦çš„ token åˆ—è¡¨
            answer_tokens = tokens[answer_start_index:answer_end_index + 1]

            # å°†ç­”æ¡ˆ token åˆ—è¡¨è½¬æ¢å›å­—ç¬¦ä¸²ï¼Œå¤„ç†ç‰¹æ®Š token å’Œå­è¯
            answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens), skip_special_tokens=True)

            print(f"\næ¨¡å‹é¢„æµ‹ç­”æ¡ˆ: {answer}")

if __name__ == "__main__":
    main()
