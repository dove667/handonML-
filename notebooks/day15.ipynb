{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e03ab8",
   "metadata": {},
   "source": [
    "# Day15 \n",
    "`Pipeline`,`AutoTokenizer`,`AutoModel`    \n",
    "\n",
    "Huggingface Transformers 基础（模型加载、Tokenizer、Pipeline 快速推理）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4082023",
   "metadata": {},
   "source": [
    "## pipeline\n",
    "\n",
    "![pipeline.png](../images/pipeline.png)\n",
    "\n",
    "包含整个流水线，从预处理到后处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66acec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f14efdd77d419792f64e940c85a603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4103d715bdb484bb7972866689cf251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b07743642748b38768e61a4c200773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7286ce62a6d4d78a0c7a05325061f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7047ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier #防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d581e97",
   "metadata": {},
   "source": [
    "这里把整个模型都加载进了内存，进行推理。（67M，F32）       \n",
    "查看模型相信信息。https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240eedb",
   "metadata": {},
   "source": [
    "Pipeline的工作流程包含以下步骤：\n",
    "\n",
    "根据输入的任务类型和 checkpoint，自动加载对应的 tokenizer 和 model\n",
    "\n",
    "- 预处理 (Preprocessing):- 将输入文本转换为模型可以理解的格式- 进行分词 (tokenization)- 添加特殊标记（如[CLS], [SEP]等）- 将token转换为对应的ID\n",
    "- 模型推理 (Model Inference):- 将处理后的输入传入预训练模型- 模型进行计算并输出原始预测结果\n",
    "- 后处理 (Post-processing):- 将模型的原始输出转换为人类可理解的格式- 对结果进行格式化（如标签和置信度分数）\n",
    "\n",
    "输入：\n",
    "\n",
    "- 可以是单个文本字符串或文本列表\n",
    "- 支持不同任务类型的特定输入格式\n",
    "\n",
    "输出：\n",
    "\n",
    "- 返回包含预测结果的字典或字典列表\n",
    "- 结果通常包含：- label: 预测的标签- score: 预测的置信度分数（0-1之间）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666a825",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "进行数据预处理和后处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ac546",
   "metadata": {},
   "source": [
    "在 Hugging Face 的 transformers 库中，AutoClass（如 AutoModel、AutoTokenizer 等）并不会在你提供的 checkpoint 名字“不完全对”时自动加载“最相近”的模型或 tokenizer。它的工作方式是基于精确匹配的逻辑，而不是模糊匹配或猜测。如果你提供的 checkpoint 名称有误，AutoClass 会尝试直接加载该名称对应的资源，如果找不到，就会抛出错误。\n",
    "\n",
    "AutoClass 的核心功能是通过 from_pretrained() 方法，根据你提供的 checkpoint 名称（通常是 Hugging Face Hub 上的模型或 tokenizer 的标识符，或者本地路径），自动推断并加载对应的模型架构或 tokenizer 类型。它的“智能”体现在以下几个方面：\n",
    "\n",
    "1. **自动推断类型**：你只需提供 checkpoint 的名称（例如 \"bert-base-uncased\"），AutoClass 会根据该 checkpoint 的配置文件（config.json）中的 model_type 字段，自动选择正确的模型类（如 BertModel）或 tokenizer 类（如 BertTokenizer）。\n",
    "2. **一致性检查**：如果 checkpoint 名称有效，AutoClass 会确保加载的模型或 tokenizer 与该 checkpoint 的配置相匹配。它不会尝试加载一个“相近”但不完全匹配的模型。\n",
    "3. **错误处理**：如果 checkpoint 名称拼写错误、不存在，或者本地缓存中没有相应的文件，AutoClass 会抛出类似 OSError 或 ValueError 的异常，提示你资源无法找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c69b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a72632451464adc89fd02a05b0f32df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172798f39b6d485aa798140ea3cc7929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb7ad2b240649fd8639257500efceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36972661dda7466e9e460dda7240d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello, how are you today? I am Grok, built by xAI!\n",
      "\n",
      "Tokenizer 输出:\n",
      "input_ids: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  2651,  1029,  1045,  2572,\n",
      "         24665,  6559,  1010,  2328,  2011,  1060,  4886,   999,   102,     0]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "token_type_ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "解码回文本: [CLS] hello, how are you today? i am grok, built by xai! [SEP] [PAD]\n",
      "分词结果: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'today', '?', 'i', 'am', 'gr', '##ok', ',', 'built', 'by', 'x', '##ai', '!', '[SEP]', '[PAD]']\n",
      "\n",
      "批量输入结果:\n",
      "input_ids: tensor([[  101,  7592,  2088,   999,   102,     0,     0],\n",
      "        [  101,  1045,  2572, 24665,  6559,  1012,   102]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "# 1. 加载 AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-base-uncased\",        # checkpoint 名称\n",
    "        use_fast=True,              # 使用快速分词器（推荐，基于 Rust 实现）\n",
    "        add_prefix_space=False,     # 是否在开头添加空格（对某些模型如 RoBERTa 有用）\n",
    ")\n",
    "    \n",
    "# 2. 输入文本\n",
    "text = \"Hello, how are you today? I am Grok, built by xAI!\"\n",
    "    \n",
    "# 3. 使用 tokenizer 处理文本（设置常用参数）\n",
    "encoded_output = tokenizer(\n",
    "    text,                       # 输入文本（字符串或字符串列表）\n",
    "    add_special_tokens=True,    # 是否添加特殊标记（如 [CLS], [SEP]）\n",
    "    max_length=20,             # 最大序列长度（超过会截断）\n",
    "    padding=\"max_length\",       # 填充到 max_length（可选：\"longest\", False 等）\n",
    "    truncation=True,            # 超过 max_length 时截断\n",
    "    return_tensors=\"pt\",        # 返回 PyTorch 张量（可选：\"tf\" 或 None）\n",
    "    return_attention_mask=True, # 返回 attention mask\n",
    "    return_token_type_ids=True, # 返回 token type IDs（用于区分句子对。在句子对任务中，第一个句子为 0，第二个句子为 1）\n",
    ")\n",
    "    \n",
    "# 4. 输出结果 字典\n",
    "print(\"原始文本:\", text)\n",
    "print(\"\\nTokenizer 输出:\")\n",
    "print(\"input_ids:\", encoded_output[\"input_ids\"])\n",
    "print(\"attention_mask:\", encoded_output[\"attention_mask\"])\n",
    "print(\"token_type_ids:\", encoded_output[\"token_type_ids\"])\n",
    "print(\"\\n解码回文本:\", tokenizer.decode(encoded_output[\"input_ids\"][0]))\n",
    "print(\"分词结果:\", tokenizer.convert_ids_to_tokens(encoded_output[\"input_ids\"][0]))\n",
    "    \n",
    "# 5. 额外功能：批量输入\n",
    "batch_text = [\"Hello world!\", \"I am Grok.\"]\n",
    "batch_encoded = tokenizer(\n",
    "    batch_text,\n",
    "    padding=True,               # 自动填充到最长序列长度\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"\\n批量输入结果:\")\n",
    "print(\"input_ids:\", batch_encoded[\"input_ids\"])\n",
    "print(\"attention_mask:\", batch_encoded[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0858b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830729cc",
   "metadata": {},
   "source": [
    "**输入参数**\n",
    "    \n",
    "1. **from_pretrained() 参数**：\n",
    "    - \"bert-base-uncased\": 指定预训练模型/checkpoint。\n",
    "    - use_fast=True: 使用 Rust 实现的快速分词器，性能更好。\n",
    "    - cache_dir: 指定缓存路径，避免重复下载。\n",
    "    - add_prefix_space: 对某些模型（如 RoBERTa）有用，BERT 不需要。\n",
    "2. **tokenizer() 参数**：\n",
    "    - text: 输入可以是单个字符串或字符串列表。\n",
    "    - add_special_tokens: 添加模型特定的标记（如 [CLS] 和 [SEP]）。\n",
    "    - max_length: 限制序列长度。\n",
    "    - padding: 填充方式（\"max_length\" 填充到指定长度，\"longest\" 填充到批次中最长序列）。\n",
    "    - truncation: 超过 max_length 时截断。\n",
    "    - return_tensors: 指定返回类型（\"pt\" 为 PyTorch，\"tf\" 为 TensorFlow，None 为 Python 列表）。\n",
    "    - return_attention_mask: 返回注意力掩码，用于区分有效 token 和填充。\n",
    "    - return_token_type_ids: 返回 token 类型 ID，用于句子对任务。\n",
    "    \n",
    "**输出内容**\n",
    "    \n",
    "1. **input_ids**：\n",
    "    - 将文本转换为 token ID 的序列，每个 ID 对应词汇表中的一个 token。\n",
    "    - [CLS]（101）和 [SEP]（102）是特殊标记，[PAD]（0）是填充。\n",
    "2. **attention_mask**：\n",
    "    - 二进制掩码，1 表示有效 token，0 表示填充。\n",
    "    - 用于告诉模型哪些部分需要关注。\n",
    "3. **token_type_ids**：\n",
    "    - 用于区分不同句子（在单句输入中通常全为 0）。\n",
    "    - 在句子对任务中，第一个句子为 0，第二个句子为 1。\n",
    "4. **解码和分词**：\n",
    "    - decode(): 将 input_ids 转换回可读文本。\n",
    "    - convert_ids_to_tokens(): 显示具体的分词结果（如 \"x\" 和 \"##ai\" 是子词）。\n",
    "    \n",
    "**批量输入**\n",
    "    \n",
    "- 当输入是列表时，padding=True 会自动对齐序列长度，填充较短的句子。\n",
    "- max_length 限制仍然有效。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a1ff7",
   "metadata": {},
   "source": [
    "**扩展功能**\n",
    "    \n",
    "1. **保存 tokenizer**：python\n",
    "        \n",
    "    ```python\n",
    "    tokenizer.save_pretrained(\"./my_tokenizer\")\n",
    "    ```\n",
    "        \n",
    "2. **处理句子对**：python\n",
    "        \n",
    "    ```python\n",
    "    encoded_pair = tokenizer(\"Hello!\", \"How are you?\", return_tensors=\"pt\")\n",
    "    print(encoded_pair[\"token_type_ids\"])  # 区分两个句子\n",
    "    ```\n",
    "        \n",
    "3. **自定义词汇表**：python\n",
    "        \n",
    "    ```python\n",
    "    tokenizer.add_tokens([\"new_token\"])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40cc4e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b133af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abafd077c14413e8ac2d62609ebeb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66e53e6bc934aec958cfd3d9385e858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599815b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f38f24",
   "metadata": {},
   "source": [
    "1. **加载模型**：\n",
    "    - 根据 checkpoint 的配置文件，自动推断模型类型（如 BertModel）。\n",
    "    - 使用 AutoModel.from_pretrained(checkpoint) 加载预训练模型。\n",
    "2. **输入数据**：\n",
    "    - AutoModel 需要接收 input_ids（来自 tokenizer 的输出）以及其他可选输入（如 attention_mask）。\n",
    "    - 输入通常是张量（PyTorch 或 TensorFlow）。\n",
    "3. **输出**：\n",
    "    - 模型返回一个包含隐藏状态（hidden states）和其他输出的对象（具体取决于模型类型和配置）。\n",
    "    - 不同任务的 AutoModel 变体返回值不同，需根据任务选择合适的类。\n",
    "    - 默认 AutoModel 不包含头部（head），仅返回隐藏状态。\n",
    "\n",
    "| **变体** | **主要输出字段** | **输出形状示例（BERT）** | **典型任务** |\n",
    "| --- | --- | --- | --- |\n",
    "| AutoModel | last_hidden_state, pooler_output | [1, 10, 768], [1, 768] | 特征提取 |\n",
    "| AutoModelForSequenceClassification | logits | [1, 2] | 文本分类 |\n",
    "| AutoModelForTokenClassification | logits | [1, 10, 3] | NER、词性标注 |\n",
    "| AutoModelForQuestionAnswering | start_logits, end_logits | [1, 10], [1, 10] | 问答 |\n",
    "| AutoModelForCausalLM | logits | [1, 10, 30522] | 文本生成（非 BERT） |\n",
    "| AutoModelForMaskedLM | logits | [1, 10, 30522] | 掩码预测 |\n",
    "| AutoModelForSeq2SeqLM | logits, encoder_last_hidden_state | [1, 10, vocab_size], [1, 10, 768] | 翻译、摘要（非 BERT） |\n",
    "\n",
    "返回logit的话到分类还要经过\n",
    "\n",
    "`torch.nn.functional.softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880534ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb14639cc004048b2c191d02bc04f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello, how are you today? I am Grok, built by xAI!\n",
      "\n",
      "Model 输出:\n",
      "最后一层隐藏状态 (last_hidden_state): torch.Size([1, 20, 768])\n",
      "池化输出 (pooler_output): torch.Size([1, 768])\n",
      "所有隐藏状态数量: 13\n",
      "注意力权重数量: 12\n",
      "\n",
      "批量输入结果:\n",
      "最后一层隐藏状态: torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. 加载 tokenizer 和 model\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint,\n",
    "    output_attentions=True,      # 返回注意力权重（可选）\n",
    "    output_hidden_states=True,   # 返回所有层的隐藏状态（可选）\n",
    ")\n",
    "\n",
    "# 2. 输入文本\n",
    "text = \"Hello, how are you today? I am Grok, built by xAI!\"\n",
    "\n",
    "# 3. 使用 tokenizer 预处理文本\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",         # 返回 PyTorch 张量\n",
    "    max_length=20,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "# 4. 将输入传递给模型\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    ")\n",
    "\n",
    "# 5. 输出结果\n",
    "print(\"原始文本:\", text)\n",
    "print(\"\\nModel 输出:\")\n",
    "print(\"最后一层隐藏状态 (last_hidden_state):\", outputs.last_hidden_state.shape)\n",
    "print(\"池化输出 (pooler_output):\", outputs.pooler_output.shape)\n",
    "print(\"所有隐藏状态数量:\", len(outputs.hidden_states))\n",
    "print(\"注意力权重数量:\", len(outputs.attentions))\n",
    "\n",
    "# 6. 批量输入示例\n",
    "batch_text = [\"Hello world!\", \"I am Grok.\"]\n",
    "batch_inputs = tokenizer(\n",
    "    batch_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "batch_outputs = model(**batch_inputs)  # 使用 ** 解包字典输入\n",
    "print(\"\\n批量输入结果:\")\n",
    "print(\"最后一层隐藏状态:\", batch_outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b4369",
   "metadata": {},
   "source": [
    "bert-base-uncased(110M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daae73",
   "metadata": {},
   "source": [
    "**输入参数**\n",
    "\n",
    "1. **from_pretrained() 参数**：\n",
    "    - checkpoint: 指定模型名称（如 \"bert-base-uncased\"）。\n",
    "    - cache_dir: 指定缓存路径。\n",
    "    - output_attentions: 是否返回注意力权重。\n",
    "    - output_hidden_states: 是否返回所有层的隐藏状态。\n",
    "2. **model() 参数**：\n",
    "    - input_ids: 来自 tokenizer 的 token ID 张量。\n",
    "    - attention_mask: 注意力掩码，区分有效 token 和填充。\n",
    "    - 可选：token_type_ids（句子对任务）、position_ids 等。\n",
    "\n",
    "**输出内容**\n",
    "\n",
    "1. **last_hidden_state**：\n",
    "    - 形状 [batch_size, sequence_length, hidden_size]。\n",
    "    - 表示最后一层的隐藏状态，每个 token 有一个 768 维向量（对于 BERT-base）。\n",
    "2. **pooler_output**：\n",
    "    - 形状 [batch_size, hidden_size]。\n",
    "    - [CLS] token 的隐藏状态经过池化层（线性 + tanh）后的输出，常用于分类任务。\n",
    "3. **hidden_states**（可选）：\n",
    "    - 一个元组，包含所有层的隐藏状态（包括嵌入层和 12 个 Transformer 层，共 13 个）。\n",
    "    - 每个形状为 [batch_size, sequence_length, hidden_size]。\n",
    "4. **attentions**（可选）：\n",
    "    - 一个元组，包含每层的注意力权重（12 层）。\n",
    "    - 每个形状为 [batch_size, num_heads, sequence_length, hidden_size]，num_heads=12。\n",
    "\n",
    "开启 output_hidden_states 或 output_attentions 会**显著增加内存使用**。\n",
    "\n",
    "**批量输入**\n",
    "\n",
    "- 输入多个句子时，padding=True 确保长度对齐。\n",
    "- 输出张量的 batch_size 变为输入样本数（这里是 2）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eccb5",
   "metadata": {},
   "source": [
    "**保存模型**：\n",
    "\n",
    "```python\n",
    "model.save_pretrained(\"./my_model\")\n",
    "```\n",
    "**GPU 支持**\n",
    "\n",
    "- 使用 .to(\"cuda\") 将模型和输入移动到 GPU。\n",
    "- 确保 PyTorch 和模型输入都在同一设备上。\n",
    "\n",
    "**冻结参数**：python\n",
    "\n",
    "```python\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False  # 不再更新\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8588ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handonML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
