{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d92d74",
   "metadata": {},
   "source": [
    "# Day12\n",
    "简单 RNN on IMDB 情感分类 (目标准确率 ≥80%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_intro",
   "metadata": {},
   "source": [
    "## 使用 huggingface 进行数据加载与预处理\n",
    "\n",
    "我们将使用 `huggingface` `datasets` 库来加载 IMDB 数据集，并进行文本的 tokenization、构建词汇表和序列填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e356f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "max_features = 10000  # 词汇表大小\n",
    "maxlen = 200          # 序列最大长度\n",
    "batch_size = 64       # 批次大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832df4f",
   "metadata": {},
   "source": [
    "datasets 库是 Hugging Face 生态系统中一个非常强大、灵活且高效的工具，专门用于加载、处理和分享数据集。它不仅仅是为 NLP 设计的，而是支持各种模态的数据（文本、图像、音频等）。   \n",
    "datasets 库加载的数据不是简单的 Python 列表或字典，而是一个专门的 Dataset 对象（或 DatasetDict 包含多个子数据集，如 train, validation, test）。\n",
    "这个 Dataset 对象基于 Apache Arrow (或 Arrow memory mapping)，可以非常高效地处理大型数据集，即使数据集大小超过你的内存容量，它也可以只加载需要的部分到内存。\n",
    "支持快速访问数据切片、按列操作等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cb12d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af32f527cb841b5a4a82b7ff6a69123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6c811f32d74e77a7ede40ba4b7b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adc19e517704bffbab506b14b1804eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cbc7808e424524b020961b6f2eb1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cfd03b11cf4fd18bc93354fc61f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd34faf8e50a435a93315eaafe7b787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c4e4f85d5f429b8ba64d439fa499f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dba6c7bad8645bf8ebb4024a907129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec4a787fbe49de9d23b866d9304657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd06c0b27f404c84a2e8b2870af3ee6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2c2c8747e84b1383f5ba708ec18d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22ad8bbf5974ad7a18e78e376ebc314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad34cb1f8e348ec9f2537f349599e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be147adedc4c849d825993323b1e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载和预处理完成。\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义预处理函数\n",
    "def preprocess_function(examples):\n",
    "    # 对文本进行 tokenization，并进行 padding 和 truncation\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=maxlen)\n",
    "\n",
    "# 应用预处理函数\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 重命名标签列并移除原始文本列\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# 设置数据格式为 PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"数据加载和预处理完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ca89e",
   "metadata": {},
   "source": [
    "datasets 提供了强大的数据处理 API，你可以用非常简洁的方式对整个数据集或数据集的某个子集进行转换操作。\n",
    "- map() 方法： 这是最核心的数据处理方法！你可以定义一个函数，然后用 dataset.map(your_function) 将这个函数应用到数据集的每个样本或每个批次上。这使得数据预处理和特征工程变得非常方便。\n",
    "- filter() 方法： 根据条件过滤数据集的样本。\n",
    "- remove_columns() / rename_columns()： 移除或重命名数据集的列。\n",
    "- select()： 选取数据集的某个子集。\n",
    "- sort()： 对数据集进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9528079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 创建完成。\n"
     ]
    }
   ],
   "source": [
    "# 创建 DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)\n",
    "\n",
    "print(\"DataLoader 创建完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be268be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本批次形状 (input_ids): torch.Size([64, 200])\n",
      "标签批次形状 (labels): torch.Size([64])\n",
      "Attention mask 批次形状: torch.Size([64, 200])\n",
      "Token type ids 批次形状: torch.Size([64, 200])\n"
     ]
    }
   ],
   "source": [
    "# 检查一个批次的数据形状\n",
    "for batch in train_dataloader:\n",
    "    print(f\"文本批次形状 (input_ids): {batch['input_ids'].shape}\")\n",
    "    print(f\"标签批次形状 (labels): {batch['labels'].shape}\")\n",
    "    # Hugging Face tokenizer 还会返回 attention_mask 和 token_type_ids (对于 BERT)\n",
    "    print(f\"Attention mask 批次形状: {batch['attention_mask'].shape}\")\n",
    "    if 'token_type_ids' in batch:\n",
    "        print(f\"Token type ids 批次形状: {batch['token_type_ids'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_model",
   "metadata": {},
   "source": [
    "## 构建简单的 LSTM 模型 \n",
    "\n",
    "搭建一个包含 Embedding 层、LSTM 层和 Dense (Linear) 层的RNN模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_rnn_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Embedding层，将词ID映射为稠密向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 单层LSTM，batch_first=True保证输入输出的第一个维度是batch\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # 全连接层，将LSTM输出映射为最终分类输出\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden: [1, batch_size, hidden_dim]，取最后一个时间步的隐藏状态\n",
    "        hidden = hidden.squeeze(0)\n",
    "        prediction = self.fc(hidden)\n",
    "        # prediction: [batch_size, output_dim]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb3121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型构建完成。\n",
      "SimpleLSTMModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 设置模型参数\n",
    "# 使用 Hugging Face tokenizer 的词汇表大小\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128 # 嵌入维度适当提升，兼顾性能和效果\n",
    "hidden_dim = 128    # LSTM 隐藏层维度适当提升，兼顾性能和效果\n",
    "output_dim = 1      # 输出维度 (二分类)\n",
    "\n",
    "# 实例化LSTM模型\n",
    "model = SimpleLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "print(\"模型构建完成。\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_training_setup",
   "metadata": {},
   "source": [
    "## 训练设置\n",
    "\n",
    "定义损失函数和优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pytorch_loss_optimizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "损失函数和优化器设置完成。\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss() # 结合 Sigmoid 和 Binary Cross Entropy Loss，更稳定\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 将模型和损失函数移动到设备 (如果可用 GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(f\"使用设备: {device}\")\n",
    "print(\"损失函数和优化器设置完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_training_loop",
   "metadata": {},
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "定义训练和评估函数，并进行模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_train_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    计算每个批次的准确率\n",
    "    \"\"\"\n",
    "    # 对预测结果进行四舍五入到最接近的整数\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() # 转换为浮点数进行除法\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # 设置模型为训练模式\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 从 Hugging Face DataLoader 的批次中提取 input_ids 和 labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # 清零梯度\n",
    "        \n",
    "        predictions = model(input_ids).squeeze(1) # 移除维度为 1 的维度\n",
    "        #保证criterion接受输入shape一致\n",
    "        loss = criterion(predictions, labels.float()) # 标签需要是浮点数\n",
    "        \n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新权重\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    \n",
    "    with torch.no_grad(): # 在评估阶段不计算梯度\n",
    "    \n",
    "        for batch in iterator:\n",
    "            # 从 Hugging Face DataLoader 的批次中提取 input_ids 和 labels\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            predictions = model(input_ids).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.644 | Train Acc: 62.61%\n",
      "\t Valid. Loss: 0.560 |  Valid. Acc: 73.64%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.433 | Train Acc: 80.95%\n",
      "\t Valid. Loss: 0.417 |  Valid. Acc: 80.87%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.291 | Train Acc: 88.50%\n",
      "\t Valid. Loss: 0.373 |  Valid. Acc: 83.77%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.213 | Train Acc: 92.15%\n",
      "\t Valid. Loss: 0.407 |  Valid. Acc: 84.17%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.148 | Train Acc: 95.11%\n",
      "\t Valid. Loss: 0.432 |  Valid. Acc: 84.06%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.88%\n",
      "\t Valid. Loss: 0.540 |  Valid. Acc: 82.78%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.068 | Train Acc: 98.15%\n",
      "\t Valid. Loss: 0.540 |  Valid. Acc: 83.11%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.66%\n",
      "\t Valid. Loss: 0.696 |  Valid. Acc: 83.07%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.95%\n",
      "\t Valid. Loss: 0.696 |  Valid. Acc: 83.42%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.28%\n",
      "\t Valid. Loss: 0.763 |  Valid. Acc: 83.09%\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "epochs = 10 # 不要继续训练了，过拟合严重\n",
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "print(\"start training...\")\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_dataloader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f'../model/SimpleLSTM/checkpoint_{epoch+1}.pth')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Valid. Loss: {valid_loss:.3f} |  Valid. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d06ebe",
   "metadata": {},
   "source": [
    "模型在epoch3之后开始过拟合。(valid loss上升)   \n",
    "可以增加dropout层，但最好在多层RNN中使用。   \n",
    "不对该单层LSTM做优化。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8102537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.373 | Test Acc: 83.77%\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型并评估\n",
    "checkpoint = torch.load(f'../model/SimpleLSTM/checkpoint_{best_epoch+1}.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29df6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面情感概率: 0.9527\n",
      "预测标签: positive\n"
     ]
    }
   ],
   "source": [
    "# 单条文本预测函数\n",
    "def predict_sentiment(model, tokenizer, sentence, device, maxlen=200):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(sentence, padding='max_length', truncation=True, max_length=maxlen, return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        prob = torch.sigmoid(output.squeeze(1)).item()\n",
    "    return prob\n",
    "\n",
    "# 示例：预测一条影评\n",
    "sample_text = \"This movie was absolutely fantastic! I loved it.\"\n",
    "prob = predict_sentiment(model, tokenizer, sample_text, device)\n",
    "print(f'正面情感概率: {prob:.4f}')\n",
    "print('预测标签:', 'positive' if prob > 0.5 else 'negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handonML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
